{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原文代码作者：François Chollet\n",
    "\n",
    "github：https://github.com/fchollet/deep-learning-with-python-notebooks\n",
    "\n",
    "中文注释制作：黄海广\n",
    "\n",
    "github：https://github.com/fengdu78\n",
    "\n",
    "代码全部测试通过。\n",
    "\n",
    "配置环境：keras 2.2.1（原文是2.0.8，运行结果一致），tensorflow 1.8，python 3.6，\n",
    "\n",
    "主机：显卡：一块1080ti；内存：32g（注：绝大部分代码不需要GPU）\n",
    "![公众号](data/gongzhong.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying movie reviews: a binary classification example\n",
    "# 电影评论分类：二分类问题\n",
    "\n",
    "This notebook contains the code samples found in Chapter 3, Section 5 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "Two-class classification, or binary classification, may be the most widely applied kind of machine learning problem. In this example, we \n",
    "will learn to classify movie reviews into \"positive\" reviews and \"negative\" reviews, just based on the text content of the reviews.\n",
    "\n",
    "二分类问题可能是应用最广泛的机器学习问题。在这个例子中，你将学习根据电影评论的文字内容将其划分为正面或负面。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The IMDB dataset\n",
    "## IMDB 数据集\n",
    "\n",
    "We'll be working with \"IMDB dataset\", a set of 50,000 highly-polarized reviews from the Internet Movie Database. They are split into 25,000 \n",
    "reviews for training and 25,000 reviews for testing, each set consisting in 50% negative and 50% positive reviews.\n",
    "\n",
    "Why do we have these two separate training and test sets? You should never test a machine learning model on the same data that you used to \n",
    "train it! Just because a model performs well on its training data doesn't mean that it will perform well on data it has never seen, and \n",
    "what you actually care about is your model's performance on new data (since you already know the labels of your training data -- obviously \n",
    "you don't need your model to predict those). For instance, it is possible that your model could end up merely _memorizing_ a mapping between \n",
    "your training samples and their targets -- which would be completely useless for the task of predicting targets for data never seen before. \n",
    "We will go over this point in much more detail in the next chapter.\n",
    "\n",
    "Just like the MNIST dataset, the IMDB dataset comes packaged with Keras. It has already been preprocessed: the reviews (sequences of words) \n",
    "have been turned into sequences of integers, where each integer stands for a specific word in a dictionary.\n",
    "\n",
    "The following code will load the dataset (when you run it for the first time, about 80MB of data will be downloaded to your machine):\n",
    "\n",
    "本节使用 IMDB 数据集，它包含来自互联网电影数据库（IMDB）的 50 000 条严重两极分化的评论。数据集被分为用于训练的 25 000 条评论与用于测试的 25 000 条评论，训练集和测试集都包含 50% 的正面评论和 50% 的负面评论。\n",
    "\n",
    "为什么要将训练集和测试集分开？因为你不应该将训练机器学习模型的同一批数据再用于测试模型！模型在训练数据上的表现很好，并不意味着它在前所未见的数据上也会表现得很好，而且你真正关心的是模型在新数据上的性能（因为你已经知道了训练数据对应的标签，显然不再需要模型来进行预测）。例如，你的模型最终可能只是记住了训练样本和目标值之间的映射关系，但这对在前所未见的数据上进行预测毫无用处。下一章将会更详细地讨论这一点。\n",
    "\n",
    "与 MNIST 数据集一样，IMDB 数据集也内置于 Keras 库。它已经过预处理：评论（单词序列） 已经被转换为整数序列，其中每个整数代表字典中的某个单词。\n",
    "下列代码将会加载 IMDB 数据集（第一次运行时会下载大约 80MB 的数据，可以不翻墙，反复试几次）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The argument `num_words=10000` means that we will only keep the top 10,000 most frequently occurring words in the training data. Rare words \n",
    "will be discarded. This allows us to work with vector data of manageable size.\n",
    "\n",
    "The variables `train_data` and `test_data` are lists of reviews, each review being a list of word indices (encoding a sequence of words). \n",
    "`train_labels` and `test_labels` are lists of 0s and 1s, where 0 stands for \"negative\" and 1 stands for \"positive\":\n",
    "\n",
    "参数 num_words=10000 的意思是仅保留训练数据中前 10 000 个最常出现的单词。低频单词将被舍弃。这样得到的向量数据不会太大，便于处理。\n",
    "\n",
    "train_data 和 test_data 这两个变量都是评论组成的列表，每条评论又是单词索引组成 的列表（表示一系列单词）。train_labels 和 test_labels 都是 0 和 1 组成的列表，其中 0 代表负面（negative），1 代表正面（positive）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 2,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 2,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 2,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we restricted ourselves to the top 10,000 most frequent words, no word index will exceed 10,000:\n",
    "\n",
    "由于限定为前 10 000 个最常见的单词，单词索引都不会超过 10 000。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([max(sequence) for sequence in train_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For kicks, here's how you can quickly decode one of these reviews back to English words:\n",
    "\n",
    "如果好奇的话，你可以用下列代码将索引解码为单词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_index is a dictionary mapping words to an integer index（word_index 是一个将单词映射为整数索引的字典）\n",
    "word_index = imdb.get_word_index()\n",
    "# We reverse it, mapping integer indices to words（键值颠倒，将整数索引映射为单词）\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "# We decode the review; note that our indices were offset by 3\n",
    "# because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\".\n",
    "#将评论解码。注意，索引减去了 3，\n",
    "# 因为 0、1、2 是为“padding”（填充）、“start of sequence”（序列开始）、“unknown”（未知词）分别保留的索引\n",
    "decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "## 准备数据\n",
    "We cannot feed lists of integers into a neural network. We have to turn our lists into tensors. There are two ways we could do that:\n",
    "\n",
    "* We could pad our lists so that they all have the same length, and turn them into an integer tensor of shape `(samples, word_indices)`, \n",
    "then use as first layer in our network a layer capable of handling such integer tensors (the `Embedding` layer, which we will cover in \n",
    "detail later in the book).\n",
    "* We could one-hot-encode our lists to turn them into vectors of 0s and 1s. Concretely, this would mean for instance turning the sequence \n",
    "`[3, 5]` into a 10,000-dimensional vector that would be all-zeros except for indices 3 and 5, which would be ones. Then we could use as \n",
    "first layer in our network a `Dense` layer, capable of handling floating point vector data.\n",
    "\n",
    "We will go with the latter solution. Let's vectorize our data, which we will do manually for maximum clarity:\n",
    "\n",
    "你不能将整数序列直接输入神经网络。你需要将列表转换为张量。转换方法有以下两种。\n",
    "\n",
    "* 填充列表，使其具有相同的长度，再将列表转换成形状为 (samples, word_indices) 的整数张量，然后网络第一层使用能处理这种整数张量的层（即 Embedding 层，本书后面会详细介绍）。\n",
    "\n",
    "* 对列表进行 one-hot 编码，将其转换为 0 和 1 组成的向量。举个例子，序列 [3, 5] 将会 被转换为 10 000 维向量，只有索引为 3 和 5 的元素是 1，其余元素都是 0。然后网络第一层可以用 Dense 层，它能够处理浮点数向量数据。\n",
    "\n",
    "下面我们采用后一种方法将数据向量化。为了加深理解，你可以手动实现这一方法，如下所示。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    # Create an all-zero matrix of shape (len(sequences), dimension)（创建一个形状为 (len(sequences), dimension) 的零矩阵）\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):#enumerate() 函数用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，\n",
    "                                            #同时列出数据和数据下标\n",
    "        results[i, sequence] = 1.  # set specific indices of results[i] to 1s（将 results[i] 的指定索引设为 1）\n",
    "    return results\n",
    "\n",
    "# Our vectorized training data（将训练数据向量化）\n",
    "x_train = vectorize_sequences(train_data)\n",
    "# Our vectorized test data（将测试数据向量化）\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what our samples look like now:\n",
    "\n",
    "样本现在变成了这样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also vectorize our labels, which is straightforward:\n",
    "\n",
    "你还应该将标签向量化，这很简单。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our vectorized labels（标签向量化）\n",
    "y_train = np.asarray(train_labels).astype('float32')  \n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our data is ready to be fed into a neural network.\n",
    "\n",
    "现在可以将数据输入到神经网络中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our network\n",
    "\n",
    "\n",
    "Our input data is simply vectors, and our labels are scalars (1s and 0s): this is the easiest setup you will ever encounter. A type of \n",
    "network that performs well on such a problem would be a simple stack of fully-connected (`Dense`) layers with `relu` activations: `Dense(16, \n",
    "activation='relu')`\n",
    "\n",
    "The argument being passed to each `Dense` layer (16) is the number of \"hidden units\" of the layer. What's a hidden unit? It's a dimension \n",
    "in the representation space of the layer. You may remember from the previous chapter that each such `Dense` layer with a `relu` activation implements \n",
    "the following chain of tensor operations:\n",
    "\n",
    "`output = relu(dot(W, input) + b)`\n",
    "\n",
    "Having 16 hidden units means that the weight matrix `W` will have shape `(input_dimension, 16)`, i.e. the dot product with `W` will project the \n",
    "input data onto a 16-dimensional representation space (and then we would add the bias vector `b` and apply the `relu` operation). You can \n",
    "intuitively understand the dimensionality of your representation space as \"how much freedom you are allowing the network to have when \n",
    "learning internal representations\". Having more hidden units (a higher-dimensional representation space) allows your network to learn more \n",
    "complex representations, but it makes your network more computationally expensive and may lead to learning unwanted patterns (patterns that \n",
    "will improve performance on the training data but not on the test data).\n",
    "\n",
    "There are two key architecture decisions to be made about such stack of dense layers:\n",
    "\n",
    "* How many layers to use.\n",
    "* How many \"hidden units\" to chose for each layer.\n",
    "\n",
    "In the next chapter, you will learn formal principles to guide you in making these choices. \n",
    "For the time being, you will have to trust us with the following architecture choice: \n",
    "two intermediate layers with 16 hidden units each, \n",
    "and a third layer which will output the scalar prediction regarding the sentiment of the current review. \n",
    "The intermediate layers will use `relu` as their \"activation function\", \n",
    "and the final layer will use a sigmoid activation so as to output a probability \n",
    "(a score between 0 and 1, indicating how likely the sample is to have the target \"1\", i.e. how likely the review is to be positive). \n",
    "A `relu` (rectified linear unit) is a function meant to zero-out negative values, \n",
    "while a sigmoid \"squashes\" arbitrary values into the `[0, 1]` interval, thus outputting something that can be interpreted as a probability.\n",
    "\n",
    "## 构建网络\n",
    "输入数据是向量，而标签是标量（1 和 0），这是你会遇到的最简单的情况。有一类网络在这种问题上表现很好， 就是带有 relu 激 活的全连接层（Dense）的简单堆叠，比如Dense(16, activation='relu')。\n",
    "\n",
    "传入 Dense 层的参数（16）是该层隐藏单元的个数。一个隐藏单元（hidden unit）是该层 表示空间的一个维度。我们在第 2 章讲过，每个带有 relu 激活的 Dense 层都实现了下列张量运算：\n",
    "\n",
    "output = relu(dot(W, input) + b)\n",
    "\n",
    "16 个隐藏单元对应的权重矩阵 W 的形状为 (input_dimension, 16)，与 W 做点积相当于将输入数据投影到 16 维表示空间中（然后再加上偏置向量 b 并应用 relu 运算）。你可以将表示空间的维度直观地理解为“网络学习内部表示时所拥有的自由度”。隐藏单元越多（即更高维的表示空间），网络越能够学到更加复杂的表示，但网络的计算代价也变得更大，而且可能会导致学到不好的模式（这种模式会提高训练数据上的性能，但不会提高测试数据上的性能）。对于这种 Dense 层的堆叠，你需要确定以下两个关键架构：\n",
    "\n",
    "* 网络有多少层；\n",
    "\n",
    "* 每层有多少个隐藏单元。\n",
    "\n",
    "下一章中的原则将会指导你对上述问题做出选择。现在你只需要相信我选择的下列架构：\n",
    "\n",
    "* 两个中间层，每层都有 16 个隐藏单元；\n",
    "\n",
    "* 第三层输出一个标量，预测当前评论的情感。\n",
    "\n",
    "中间层使用 relu 作为激活函数，最后一层使用 sigmoid 激活以输出一个 0~1 范围内的概率值（表示样本的目标值等于 1 的可能性，即评论为正面的可能性）。relu（rectified linear unit，整流线性单元）函数将所有负值归零，而 sigmoid 函数则将任意值“压缩”到 [0,1] 区间内，其输出值可以看作概率值。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what our network looks like:\n",
    "\n",
    "网络架构如下：\n",
    "\n",
    "![3-layer network](data/3_layer_network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the Keras implementation, very similar to the MNIST example you saw previously:\n",
    "\n",
    "代码清单是其 Keras 实现，与前面见过的 MNIST 例子类似。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Lastly, we need to pick a loss function and an optimizer. Since we are facing a binary classification problem and the output of our network \n",
    "is a probability (we end our network with a single-unit layer with a sigmoid activation), is it best to use the `binary_crossentropy` loss. \n",
    "It isn't the only viable choice: you could use, for instance, `mean_squared_error`. But crossentropy is usually the best choice when you \n",
    "are dealing with models that output probabilities. Crossentropy is a quantity from the field of Information Theory, that measures the \"distance\" \n",
    "between probability distributions, or in our case, between the ground-truth distribution and our predictions.\n",
    "\n",
    "Here's the step where we configure our model with the `rmsprop` optimizer and the `binary_crossentropy` loss function. Note that we will \n",
    "also monitor accuracy during training.\n",
    "\n",
    "最后，你需要选择损失函数和优化器。由于你面对的是一个二分类问题，网络输出是一个概率值（网络最后一层使用 sigmoid 激活函数，仅包含一个单元），那么最好使用 `binary_crossentropy` （二元交叉熵）损失。这并不是唯一可行的选择，比如你还可以使用 `mean_squared_error`（均方误差）。但对于输出概率值的模型，交叉熵（crossentropy）往往是最好的选择。交叉熵是来自于信息论领域的概念，用于衡量概率分布之间的距离，在这个例子中就是真实分布与预测值之间的距离。\n",
    "\n",
    "下面的步骤是用 rmsprop 优化器和 `binary_crossentropy`  损失函数来配置模型。注意，我们还在训练过程中监控精度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are passing our optimizer, loss function and metrics as strings, which is possible because `rmsprop`, `binary_crossentropy` and \n",
    "`accuracy` are packaged as part of Keras. Sometimes you may want to configure the parameters of your optimizer, or pass a custom loss \n",
    "function or metric function. This former can be done by passing an optimizer class instance as the `optimizer` argument:\n",
    "\n",
    "上述代码将优化器、损失函数和指标作为字符串传入，这是因为 rmsprop、binary_ crossentropy 和 accuracy 都是 Keras 内置的一部分。有时你可能希望配置自定义优化器的 参数，或者传入自定义的损失函数或指标函数。前者可通过向 optimizer 参数传入一个优化器类实例来实现，如代码所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latter can be done by passing function objects as the `loss` or `metrics` arguments:\n",
    "\n",
    "后者可通过向 loss 和 metrics 参数传入函数对象来实现， 如代码所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import losses\n",
    "from keras import metrics\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "              loss=losses.binary_crossentropy,\n",
    "              metrics=[metrics.binary_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating our approach\n",
    "\n",
    "In order to monitor during training the accuracy of the model on data that it has never seen before, we will create a \"validation set\" by \n",
    "setting apart 10,000 samples from the original training data:\n",
    "\n",
    "## 验证你的方法\n",
    "为了在训练过程中监控模型在前所未见的数据上的精度，你需要将原始训练数据留出 10 000个样本作为验证集。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We will now train our model for 20 epochs (20 iterations over all samples in the `x_train` and `y_train` tensors), in mini-batches of 512 \n",
    "samples. At this same time we will monitor loss and accuracy on the 10,000 samples that we set apart. This is done by passing the \n",
    "validation data as the `validation_data` argument:\n",
    "\n",
    "现在使用 512 个样本组成的小批量，将模型训练 20 个轮次（即对 x_train 和 y_train 两 个张量中的所有样本进行 20 次迭代）。与此同时，你还要监控在留出的 10 000 个样本上的损失和精度。你可以通过将验证数据传入 validation_data 参数来完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "15000/15000 [==============================] - 4s 243us/step - loss: 0.5084 - binary_accuracy: 0.7816 - val_loss: 0.3797 - val_binary_accuracy: 0.8679\n",
      "Epoch 2/20\n",
      "15000/15000 [==============================] - 2s 123us/step - loss: 0.3002 - binary_accuracy: 0.9045 - val_loss: 0.3002 - val_binary_accuracy: 0.8899\n",
      "Epoch 3/20\n",
      "15000/15000 [==============================] - 2s 111us/step - loss: 0.2179 - binary_accuracy: 0.9285 - val_loss: 0.3086 - val_binary_accuracy: 0.8714\n",
      "Epoch 4/20\n",
      "15000/15000 [==============================] - 2s 115us/step - loss: 0.1750 - binary_accuracy: 0.9437 - val_loss: 0.2839 - val_binary_accuracy: 0.8830\n",
      "Epoch 5/20\n",
      "15000/15000 [==============================] - 2s 114us/step - loss: 0.1427 - binary_accuracy: 0.9543 - val_loss: 0.2848 - val_binary_accuracy: 0.8864\n",
      "Epoch 6/20\n",
      "15000/15000 [==============================] - 2s 112us/step - loss: 0.1150 - binary_accuracy: 0.9650 - val_loss: 0.3132 - val_binary_accuracy: 0.8789\n",
      "Epoch 7/20\n",
      "15000/15000 [==============================] - 2s 115us/step - loss: 0.0980 - binary_accuracy: 0.9707 - val_loss: 0.3127 - val_binary_accuracy: 0.8845\n",
      "Epoch 8/20\n",
      "15000/15000 [==============================] - 2s 118us/step - loss: 0.0808 - binary_accuracy: 0.9762 - val_loss: 0.3857 - val_binary_accuracy: 0.8653\n",
      "Epoch 9/20\n",
      "15000/15000 [==============================] - 2s 122us/step - loss: 0.0661 - binary_accuracy: 0.9820 - val_loss: 0.3631 - val_binary_accuracy: 0.8780\n",
      "Epoch 10/20\n",
      "15000/15000 [==============================] - 2s 120us/step - loss: 0.0558 - binary_accuracy: 0.9851 - val_loss: 0.3840 - val_binary_accuracy: 0.8790\n",
      "Epoch 11/20\n",
      "15000/15000 [==============================] - 3s 167us/step - loss: 0.0449 - binary_accuracy: 0.9888 - val_loss: 0.4160 - val_binary_accuracy: 0.8768\n",
      "Epoch 12/20\n",
      "15000/15000 [==============================] - 2s 146us/step - loss: 0.0385 - binary_accuracy: 0.9915 - val_loss: 0.4506 - val_binary_accuracy: 0.8696\n",
      "Epoch 13/20\n",
      "15000/15000 [==============================] - 2s 135us/step - loss: 0.0300 - binary_accuracy: 0.9930 - val_loss: 0.4695 - val_binary_accuracy: 0.8729\n",
      "Epoch 14/20\n",
      "15000/15000 [==============================] - 2s 121us/step - loss: 0.0246 - binary_accuracy: 0.9948 - val_loss: 0.5011 - val_binary_accuracy: 0.8721\n",
      "Epoch 15/20\n",
      "15000/15000 [==============================] - 2s 130us/step - loss: 0.0176 - binary_accuracy: 0.9981 - val_loss: 0.5475 - val_binary_accuracy: 0.8674\n",
      "Epoch 16/20\n",
      "15000/15000 [==============================] - 2s 129us/step - loss: 0.0151 - binary_accuracy: 0.9977 - val_loss: 0.5890 - val_binary_accuracy: 0.8661\n",
      "Epoch 17/20\n",
      "15000/15000 [==============================] - 2s 131us/step - loss: 0.0113 - binary_accuracy: 0.9989 - val_loss: 0.6153 - val_binary_accuracy: 0.8674\n",
      "Epoch 18/20\n",
      "15000/15000 [==============================] - 2s 124us/step - loss: 0.0120 - binary_accuracy: 0.9973 - val_loss: 0.6422 - val_binary_accuracy: 0.8679\n",
      "Epoch 19/20\n",
      "15000/15000 [==============================] - 2s 120us/step - loss: 0.0055 - binary_accuracy: 0.9997 - val_loss: 0.7295 - val_binary_accuracy: 0.8547\n",
      "Epoch 20/20\n",
      "15000/15000 [==============================] - 2s 126us/step - loss: 0.0100 - binary_accuracy: 0.9975 - val_loss: 0.7018 - val_binary_accuracy: 0.8657\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On CPU, this will take less than two seconds per epoch -- training is over in 20 seconds. At the end of every epoch, there is a slight pause \n",
    "as the model computes its loss and accuracy on the 10,000 samples of the validation data.\n",
    "\n",
    "Note that the call to `model.fit()` returns a `History` object. This object has a member `history`, which is a dictionary containing data \n",
    "about everything that happened during training. Let's take a look at it:\n",
    "\n",
    "在 CPU 上运行，每轮的时间不到 2 秒，训练过程将在 20 秒内结束。每轮结束时会有短暂的停顿，因为模型要计算在验证集的 10 000 个样本上的损失和精度。\n",
    "注意，调用 model.fit() 返回了一个 History 对象。这个对象有一个成员 history，它是一个字典，包含训练过程中的所有数据。我们来看一下。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_binary_accuracy', 'loss', 'binary_accuracy'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It contains 4 entries: one per metric that was being monitored, during training and during validation. Let's use Matplotlib to plot the \n",
    "training and validation loss side by side, as well as the training and validation accuracy:\n",
    "\n",
    "字典中包含 4 个条目，对应训练过程和验证过程中监控的指标。在下面两个代码清单中， 我们将使用 Matplotlib 在同一张图上绘制训练损失和验证损失，以及训练精度和验证精度）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['binary_accuracy']\n",
    "val_acc = history.history['val_binary_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"（'bo' 表示蓝色圆点）\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"（'b' 表示蓝色实线）\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()   # clear figure（清空图像）\n",
    "acc_values = history_dict['binary_accuracy']\n",
    "val_acc_values = history_dict['val_binary_accuracy']\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The dots are the training loss and accuracy, while the solid lines are the validation loss and accuracy. Note that your own results may vary \n",
    "slightly due to a different random initialization of your network.\n",
    "\n",
    "As you can see, the training loss decreases with every epoch and the training accuracy increases with every epoch. That's what you would \n",
    "expect when running gradient descent optimization -- the quantity you are trying to minimize should get lower with every iteration. But that \n",
    "isn't the case for the validation loss and accuracy: they seem to peak at the fourth epoch. This is an example of what we were warning \n",
    "against earlier: a model that performs better on the training data isn't necessarily a model that will do better on data it has never seen \n",
    "before. In precise terms, what you are seeing is \"overfitting\": after the second epoch, we are over-optimizing on the training data, and we \n",
    "ended up learning representations that are specific to the training data and do not generalize to data outside of the training set.\n",
    "\n",
    "In this case, to prevent overfitting, we could simply stop training after three epochs. In general, there is a range of techniques you can \n",
    "leverage to mitigate overfitting, which we will cover in the next chapter.\n",
    "\n",
    "Let's train a new network from scratch for four epochs, then evaluate it on our test data:\n",
    "\n",
    "点是训练损失和准确率，而实线是验证损失和准确性。 请注意，由于网络的随机初始化不同，您自己的结果可能略有不同。\n",
    "\n",
    "如你所见，训练损失每轮都在降低，训练精度每轮都在提升。这就是梯度下降优化的预期 结果——你想要最小化的量随着每次迭代越来越小。但验证损失和验证精度并非如此：它们似 乎在第四轮达到最佳值。这就是我们之前警告过的一种情况：模型在训练数据上的表现越来越好， 但在前所未见的数据上不一定表现得越来越好。准确地说，你看到的是过拟合（overfit）：在第二轮之后，你对训练数据过度优化，最终学到的表示仅针对于训练数据，无法泛化到训练集之外的数据。\n",
    "\n",
    "在这种情况下，为了防止过拟合，你可以在 3 轮之后停止训练。通常来说，你可以使用许 多方法来降低过拟合，我们将在第 4 章中详细介绍。\n",
    "\n",
    "我们从头开始训练一个新的网络，训练 4 轮，然后在测试数据上评估模型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 2s 95us/step - loss: 0.4743 - acc: 0.8218\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 2s 75us/step - loss: 0.2662 - acc: 0.9099\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 2s 80us/step - loss: 0.1984 - acc: 0.9296\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 2s 77us/step - loss: 0.1678 - acc: 0.9399\n",
      "25000/25000 [==============================] - 3s 121us/step\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
    "results = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.32367459528923037, 0.87376]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our fairly naive approach achieves an accuracy of 88%. With state-of-the-art approaches, one should be able to get close to 95%.\n",
    "\n",
    "这种相当简单的方法得到了 88% 的精度。利用最先进的方法，你应该能够得到接近 95% 的 精度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a trained network to generate predictions on new data\n",
    "\n",
    "After having trained a network, you will want to use it in a practical setting. You can generate the likelihood of reviews being positive \n",
    "by using the `predict` method:\n",
    "\n",
    "## 使用训练好的网络在新数据上生成预测结果\n",
    "\n",
    "训练好网络之后，你希望将其用于实践。你可以用 predict 方法来得到评论为正面的可能性大小。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13771522],\n",
       "       [0.99972296],\n",
       "       [0.29092878],\n",
       "       ...,\n",
       "       [0.07194319],\n",
       "       [0.04204583],\n",
       "       [0.4775203 ]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the network is very confident for some samples (0.99 or more, or 0.01 or less) but less confident for others (0.6, 0.4). \n",
    "\n",
    "如你所见，网络对某些样本的结果非常确信（大于等于 0.99，或小于等于 0.01），但对其他结果却不那么确信（0.6 或 0.4）。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further experiments\n",
    "\n",
    "\n",
    "* We were using 2 hidden layers. Try to use 1 or 3 hidden layers and see how it affects validation and test accuracy.\n",
    "* Try to use layers with more hidden units or less hidden units: 32 units, 64 units...\n",
    "* Try to use the `mse` loss function instead of `binary_crossentropy`.\n",
    "* Try to use the `tanh` activation (an activation that was popular in the early days of neural networks) instead of `relu`.\n",
    "\n",
    "These experiments will help convince you that the architecture choices we have made are all fairly reasonable, although they can still be \n",
    "improved!\n",
    "\n",
    "## 进一步的实验\n",
    "通过以下实验，你可以确信前面选择的网络架构是非常合理的，虽然仍有改进的空间。\n",
    "\n",
    "* 前面使用了两个隐藏层。你可以尝试使用一个或三个隐藏层，然后观察对验证精度和测试精度的影响。\n",
    "\n",
    "* 尝试使用更多或更少的隐藏单元，比如 32 个、64 个等。\n",
    "\n",
    "* 尝试使用 mse 损失函数代替 binary_crossentropy。\n",
    "\n",
    "* 尝试使用 tanh 激活（这种激活在神经网络早期非常流行）代替 relu。\n",
    "\n",
    "这些实验将有助于说服您，我们所做的架构选择都是相当合理的，尽管它们仍然可以改进！\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "\n",
    "Here's what you should take away from this example:\n",
    "\n",
    "* There's usually quite a bit of preprocessing you need to do on your raw data in order to be able to feed it -- as tensors -- into a neural \n",
    "network. In the case of sequences of words, they can be encoded as binary vectors -- but there are other encoding options too.\n",
    "* Stacks of `Dense` layers with `relu` activations can solve a wide range of problems (including sentiment classification), and you will \n",
    "likely use them frequently.\n",
    "* In a binary classification problem (two output classes), your network should end with a `Dense` layer with 1 unit and a `sigmoid` activation, \n",
    "i.e. the output of your network should be a scalar between 0 and 1, encoding a probability.\n",
    "* With such a scalar sigmoid output, on a binary classification problem, the loss function you should use is `binary_crossentropy`.\n",
    "* The `rmsprop` optimizer is generally a good enough choice of optimizer, whatever your problem. That's one less thing for you to worry \n",
    "about.\n",
    "* As they get better on their training data, neural networks eventually start _overfitting_ and end up obtaining increasingly worse results on data \n",
    "never-seen-before. Make sure to always monitor performance on data that is outside of the training set.\n",
    "\n",
    "## 小结\n",
    "下面是你应该从这个例子中学到的要点。\n",
    "\n",
    "* 通常需要对原始数据进行大量预处理，以便将其转换为张量输入到神经网络中。单词序列可以编码为二进制向量，但也有其他编码方式。\n",
    "\n",
    "* 带有 relu 激活的 Dense 层堆叠，可以解决很多种问题（包括情感分类），你可能会经常用到这种模型。\n",
    "\n",
    "* 对于二分类问题（两个输出类别），网络的最后一层应该是只有一个单元并使用 sigmoid激活的 Dense 层，网络输出应该是 0~1 范围内的标量，表示概率值。\n",
    "\n",
    "* 对于二分类问题的 sigmoid 标量输出，你应该使用 binary_crossentropy 损失函数。\n",
    "\n",
    "* 无论你的问题是什么，rmsprop 优化器通常都是足够好的选择。这一点你无须担心。\n",
    "\n",
    "* 随着神经网络在训练数据上的表现越来越好，模型最终会过拟合，并在前所未见的数据上得到越来越差的结果。一定要一直监控模型在训练集之外的数据上的性能。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
